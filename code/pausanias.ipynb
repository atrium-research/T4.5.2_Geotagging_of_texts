{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45066fd4",
   "metadata": {},
   "source": [
    "# Subtask 4.5.2 Geotagging of texts\n",
    "\n",
    "This subtask is part of the [ATRIUM](https://atrium-research.eu/) project. The data we use is from the [Digital Periegesis](https://www.periegesis.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff9f0d",
   "metadata": {},
   "source": [
    "### Step 0: Download and import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a6f59",
   "metadata": {},
   "source": [
    "For Ollama to work you need to download the client and then open it (leave it running on the background).\n",
    "\n",
    "* Download [Ollama client](https://ollama.com/download).\n",
    "* After installing Ollama, run on your cmd this `ollama pull qwen3:14b`.\n",
    "* You can find this model [here](https://ollama.com/library/qwen3).\n",
    "* If you want better results and have the computational capacity to support larger models, then you can choose one from this [list](https://ollama.com/search).\n",
    "\n",
    "* For the embedding model we used the [Qwen-Embedding-8B](https://ollama.com/library/qwen3-embedding:8b).\n",
    "* Run on your cmd this `ollama pull qwen3-embedding:8b`.\n",
    "\n",
    "\n",
    "In order to use [torch](https://pytorch.org/get-started/locally/) you need to determine if you have an Nvidia GPU with CUDA 12.6+, or if you have a CPU.\n",
    "\n",
    "* It is highly recommened to use a GPU with CUDA 12.6+.\n",
    "* For this notebook we will use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from ollama import chat,ChatResponse\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "os.makedirs(\"files\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"books\", exist_ok=True)\n",
    "os.makedirs(\"books_chapters\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eee26c",
   "metadata": {},
   "source": [
    "### Step 1: Extract the texts from Digital Periegisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60682da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "\n",
    "for number in tqdm(range(1, 358)):\n",
    "    response = requests.get(f\"https://www.periegesis.org/en/reports.php?reportid={number}\", headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    x = soup.find(\"div\", class_=\"text_max_width\")\n",
    "\n",
    "    for tag in soup.find_all([\"b\", \"strong\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "    chapter = soup.find(\"h2\", class_=\"head\").text\n",
    "    book = soup.find(\"h5\", class_=\"head align_center\").text\n",
    "\n",
    "    for i in x.find_all(\"p\"):\n",
    "        metadata = []\n",
    "        clean_text = re.split(r\"BOOK\\s+\\d+\", i.text)[-1].strip()\n",
    "        for pl in i.find_all(\"pl\"):\n",
    "            if pl.get(\"id\").startswith(\"Q\"):\n",
    "                metadata.append({\"chapter\":int(chapter.split()[-1]),\n",
    "                                \"book\":int(book.split()[-1])})\n",
    "                \n",
    "        final_list.append({\"text\":clean_text,\"metadata\":metadata})\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "srsly.write_jsonl(f\"files/1_pausanias.jsonl\",final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978e140",
   "metadata": {},
   "source": [
    "### Step 2: Perform Name Entity Recognition (NER) using [NameTag 3](https://lindat.mff.cuni.cz/services/nametag/) service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(srsly.read_jsonl(\"files/1_pausanias.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in tqdm(data):\n",
    "    x = requests.get(f\"https://lindat.mff.cuni.cz/services/nametag/api/recognize?data={element.get(\"text\")}&model=nametag3-multilingual-conll-250203&output=vertical\", headers=headers).json()\n",
    "    new_element = [i.split(\"\\t\")[-1].replace(\"\\xad\",\"\") for i in x.get(\"result\").splitlines() if i.split(\"\\t\")[1] != \"PER\"]\n",
    "\n",
    "    element.update({\"nametag_mentions\":new_element})\n",
    "\n",
    "    srsly.write_jsonl(\"files/2_pausanias_nametag.jsonl\", [element], append=True, append_new_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57051bf7",
   "metadata": {},
   "source": [
    "### Step 3: Recontext the NER predictions using a Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the instructions the LLM will use, you can change it according to your task\n",
    "\n",
    "# prompt = \"Your task is to...\"\n",
    "\n",
    "prompt = 'You are a professional ancient Greek historian.' \\\n",
    "'You will be given: A mention (e.g., stoa, “temple,” “grave,” “harbor”) along with a text from Pausanias.' \\\n",
    "'Your task: Determine exactly what the mention refers to in context, paying attention to qualifiers such as “largest,” “near,” “old,” ' \\\n",
    "'as well as the subject matter of decorations, inscriptions, or dedications that can uniquely identify the place, structure, or object. ' \\\n",
    "'Write a concise, 2-sentence Wikipedia-style entry describing the place, including historical and archaeological context. ' \\\n",
    "'If multiple candidates exist (e.g., several harbors), specify the particular one the text refers to. ' \\\n",
    "'Output format example with three cases: ' \\\n",
    "'Example 1 Mention: building Text: “On entering the city there is a building for the preparation of the processions, ' \\\n",
    "'which are held in some cases every year, in others at longer intervals. Hard by is a temple of Demeter, with images of the goddess herself ' \\\n",
    "'and of her daughter, and of Iacchus holding a torch. On the wall, in Attic characters, is written that they are works of Praxiteles. ' \\\n",
    "'Not far from the temple is Poseidon on horseback, hurling a spear against the giant Polybotes … ' \\\n",
    "'From the gate to the Cerameicus there are stoas, and in front of them bronze statues of such as had some title to fame, both men and women.” ' \\\n",
    "'Output: Building (Pompeion): The building is the Pompeion, located in ancient Athens between the Dipylus and the Holy Gate, ' \\\n",
    "'west of the Ancient Agora and the first building of the inner Kerameikos. ' \\\n",
    "'It was used for the preparation of processions, including those of the Panathenaia and other festivals. ' \\\n",
    "'Construction appears to have begun in the 5th century BCE and was largely completed by the early 4th century BCE. ' \\\n",
    "'Example 2 Mention: shipsheds Text: “Even up to my time there were shipsheds there, and near the largest harbor is the grave of Themistocles.” ' \\\n",
    "'Output: shipsheds: The shipsheds of Piraeus were covered naval storage structures used to house and maintain the Athenian fleet, ' \\\n",
    "'protecting triremes from weather and decay. They were built during the early 5th century BCE as part of Themistocles’ expansion of Piraeus' \\\n",
    "' and were located near Kantharos, the largest harbor. These shipsheds remained standing into Pausanias’ time, symbolizing the city’s ' \\\n",
    "'enduring naval infrastructure. ' \\\n",
    "'Example 3 Mention: harbor Text: “Even up to my time there were shipsheds there, and near the largest harbor is the grave of Themistocles.” ' \\\n",
    "'Output: harbor: Kantharos was the largest of the three harbors of Piraeus and served as the main port of ancient ' \\\n",
    "'Athens after Themistocles’ expansion around 493 BCE. It provided docking, loading, and maintenance facilities for the Athenian ' \\\n",
    "'fleet and was central to the city’s naval power and trade. Pausanias notes that near Kantharos were the shipsheds and' \\\n",
    "' the grave of Themistocles, highlighting its historical and maritime significance.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(srsly.read_jsonl(\"files/2_pausanias_nametag.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba31ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen3:14b\" #change this to your prefered model from Ollama (https://ollama.com/search)\n",
    "\n",
    "for i in tqdm(data):\n",
    "    lista = []\n",
    "    text = i.get(\"text\")\n",
    "    search = 0\n",
    "    for mention in i.get(\"nametag_mentions\"):\n",
    "      response: ChatResponse = chat(model=model, messages=[\n",
    "        {\n",
    "          'role': 'system',\n",
    "          'content': prompt,\n",
    "        },\n",
    "        {\n",
    "          'role': 'user',\n",
    "          'content': f'Mention: {mention}, Sentence: {text}',\n",
    "        },\n",
    "      ],\n",
    "      think=False)\n",
    "\n",
    "      start = text.find(mention, search)\n",
    "      end = start + len(mention)\n",
    "      search = end\n",
    "\n",
    "      lista.append({\"name\":mention, \"start\":start, \"end\":end, \"recontext\":response.message.content})\n",
    "\n",
    "    i.update({\"mentions_tagged\":lista})\n",
    "    \n",
    "    srsly.write_jsonl(\"files/3_pausanias_ner.jsonl\", [i], append=True, append_new_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f33c4",
   "metadata": {},
   "source": [
    "### Step 4: Use a [FAISS index](https://github.com/facebookresearch/faiss) for fast approximate retrieval [(HNSW)](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world)\n",
    "\n",
    "* The FAISS index was built from [ToposText](https://topostext.org/) database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cc4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pausanias_data = list(srsly.read_jsonl(\"files/3_pausanias_ner.jsonl\"))\n",
    "data = srsly.read_json(\"data/ToposText_gazetteer.json\")\n",
    "\n",
    "index = faiss.read_index(\"data/topostext.index\")\n",
    "with open(\"data/topostext_meta.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(pausanias_data):\n",
    "    for mention in i.get(\"mentions_tagged\"):\n",
    "        lista = []\n",
    "        query_text = f\"{mention.get(\"name\")}: {mention.get(\"recontext\")}\"\n",
    "        x_query = ollama.embed(model='qwen3-embedding:8b', input=query_text) # The faiss was built with qwen3-embedding:8b\n",
    "        query_vec = np.array(x_query[\"embeddings\"], dtype=np.float32)\n",
    "        \n",
    "        # Comment this if you want to get the top 1 result without running the 4.1 step\n",
    "        distances, indices = index.search(query_vec, 100)\n",
    "\n",
    "        # Uncomment this if you want to the the top 1 result without running the 4.1 step\n",
    "        #distances, indices = index.search(query_vec, 1)\n",
    "\n",
    "        result_ids = metadata.get(\"ids\")[indices]\n",
    "        for id,distance in zip(result_ids[0], distances[0]):\n",
    "            lista.append([data[\"features\"][id].get(\"@id\").split(\"/\")[-1], str(distance)])\n",
    "\n",
    "        mention.update({\"vector_db\":lista})\n",
    "    srsly.write_jsonl(\"files/4_pausanias_faiss.jsonl\", [i], append=True, append_new_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a66037",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Step 4.1: Run a Reranker for better results\n",
    "\n",
    "* We are using the [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B).\n",
    "* You can use either the [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B) if you have the resources.\n",
    "* Or the [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) if you have limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_model = \"Qwen/Qwen3-Reranker-4B\" #change this depending if you want the 4b, the 8b or the 0.6b (e.g. 'Qwen/Qwen3-Reranker-8B', 'Qwen/Qwen3-Reranker-0.6B')\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = \"Determine whether the Document describes or identifies the same historical place, group, or location referred to in the Query. Answer 'yes' only if it clearly refers to that exact entity, not to a nearby site or people associated with it. Otherwise, answer 'no'.\"\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model, padding_side='left')\n",
    "\n",
    "# FOR GPU - We recommend enabling flash_attention_2 or sdpa for better acceleration and memory saving.\n",
    "#model = AutoModelForCausalLM.from_pretrained(rerank_model, torch_dtype=torch.float16, attn_implementation=\"sdpa\").to('cuda').eval()\n",
    "\n",
    "# FOR CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(rerank_model).eval()\n",
    "\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"You are a factual judge. Using the Instruct and the Query, decide whether the Document \"\n",
    "    \"clearly describes the same historical place, group, or location. \"\n",
    "    \"Answer only with 'yes' or 'no'.\\n\"\n",
    "    \"<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n\"\n",
    ")\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topos_data = list(srsly.read_jsonl(\"files/4_pausanias_faiss.jsonl\"))\n",
    "data = srsly.read_json(\"data/ToposText_gazetteer.json\")\n",
    "data = data[\"features\"]\n",
    "\n",
    "task = \"Determine whether the Document describes or identifies the same historical place, group, or location referred to in the Query. Answer 'yes' only if it clearly refers to that exact entity, not to a nearby site or people associated with it. Otherwise, answer 'no'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(topos_data):\n",
    "    for mention in i.get(\"mentions_tagged\"):\n",
    "        topos_text_ids = [i[0] for i in mention.get(\"vector_db\")][:30]\n",
    "        hash_map = []\n",
    "        final_list = []\n",
    "\n",
    "        for id in topos_text_ids:\n",
    "            for d in data:\n",
    "                if d.get(\"@id\").split(\"/\")[-1] == id:\n",
    "                    title = d.get(\"properties\").get(\"title\")\n",
    "                    description = d.get(\"properties\").get(\"description\")\n",
    "                    hash_map.append({id:f\"{title}: {description}\"})\n",
    "                    break\n",
    "\n",
    "        for element in hash_map:\n",
    "            query = f\"The mention '{mention.get(\"name\")}' appears in the context: {mention.get(\"recontext\")}\"\n",
    "            pairs = [format_instruction(task, query, list(element.values())[0])]\n",
    "\n",
    "            inputs = process_inputs(pairs)\n",
    "            scores = compute_logits(inputs)\n",
    "\n",
    "            final_list.append({list(element.keys())[0]:scores[0]})\n",
    "        \n",
    "        final_list = sorted(final_list, key=lambda x: list(x.values())[0], reverse=True)[0]\n",
    "        mention.update({\"reranker\":[final_list]})\n",
    "    \n",
    "    srsly.write_jsonl(\"files/4_1_pausanias_rerank.jsonl\", [i], append=True, append_new_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d284acc",
   "metadata": {},
   "source": [
    "### Step 5: Create the input for the [Recogito Studio](https://recogitostudio.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that we executed the 4.1 optional step, comment this if you didn't execute it\n",
    "data = list(srsly.read_jsonl(\"files/4_1_pausanias_rerank.jsonl\"))\n",
    "\n",
    "# Uncomment this if you didn't execute the 4.1 optional step\n",
    "#data = list(srsly.read_jsonl(\"files/4_pausanias_faiss.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7140176",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS_TEI = \"http://www.tei-c.org/ns/1.0\"\n",
    "NS_XML = \"http://www.w3.org/XML/1998/namespace\"\n",
    "NSMAP = {None: NS_TEI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f025d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "uid_counter = 0\n",
    "chapter = 0\n",
    "current_book = None\n",
    "current_chapter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e29a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want each chapter-book pair to be a different XML run this cell\n",
    "for i in data:\n",
    "    book = i.get(\"book\")\n",
    "    chapter = i.get(\"chapter\")\n",
    "\n",
    "    if current_chapter is not None and chapter != current_chapter:\n",
    "        tree = etree.ElementTree(tei)\n",
    "        tree.write(\n",
    "            f\"books_chapters/pausanias_book_{current_book}_chapter_{current_chapter}.xml\",\n",
    "            xml_declaration=True,\n",
    "            encoding=\"utf-8\",\n",
    "            pretty_print=True\n",
    "        )\n",
    "\n",
    "        tei = etree.Element(\"TEI\", nsmap=NSMAP, version=\"3.3.0\")\n",
    "        standoff = etree.SubElement(tei, \"standOff\", type=\"recogito_studio_annotations\")\n",
    "        listannotation = etree.SubElement(standoff, \"listAnnotation\")\n",
    "        text = etree.SubElement(tei, \"text\")\n",
    "        body = etree.SubElement(text, \"body\")\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        head = etree.SubElement(body, \"head\")\n",
    "        head.text = f\"Book {book}, Chapter {chapter}\"\n",
    "\n",
    "    if current_chapter is None:\n",
    "        tei = etree.Element(\"TEI\", nsmap=NSMAP, version=\"3.3.0\")\n",
    "        standoff = etree.SubElement(tei, \"standOff\", type=\"recogito_studio_annotations\")\n",
    "        listannotation = etree.SubElement(standoff, \"listAnnotation\")\n",
    "        text = etree.SubElement(tei, \"text\")\n",
    "        body = etree.SubElement(text, \"body\")\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        head = etree.SubElement(body, \"head\")\n",
    "        head.text = f\"Book {book}, Chapter {chapter}\"\n",
    "\n",
    "    current_book = book\n",
    "    current_chapter = chapter\n",
    "\n",
    "    p = etree.SubElement(body, \"p\")\n",
    "    p.text = i.get(\"text\")\n",
    "\n",
    "    for mention in i.get(\"mentions_tagged\"):\n",
    "        annotation = etree.SubElement(listannotation, \"annotation\", target=f\"/TEI[1]/text[1]/body[1]/p[{str(counter)}]::{str(mention.get(\"start\"))} /TEI[1]/text[1]/body[1]/p[{str(counter)}]::{str(mention.get(\"end\"))}\")\n",
    "        annotation.set(f\"{{{NS_XML}}}id\", f\"UID-FAKE-{uid_counter}\")\n",
    "\n",
    "        # Comment this if you didn't run the optional step\n",
    "        topos_id = list(mention.get(\"reranker\")[0].keys())[0]\n",
    "        \n",
    "        # Uncomment this if you didn't run the optional step\n",
    "        #topos_id = mention.get(\"vector_db\")[0][0]\n",
    "        \n",
    "        rs = etree.SubElement(annotation, \"rs\", ana=f\"https://topostext.org/place/{topos_id}\")\n",
    "        uid_counter += 1\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "tree = etree.ElementTree(tei)\n",
    "tree.write(\n",
    "    f\"books_chapters/pausanias_book_{current_book}_chapter_{current_chapter}.xml\",\n",
    "    xml_declaration=True,\n",
    "    encoding=\"utf-8\",\n",
    "    pretty_print=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want each book to be a different XML run this\n",
    "for i in data:\n",
    "    if current_book is not None and i.get(\"book\") != current_book:\n",
    "        tree = etree.ElementTree(tei)\n",
    "        tree.write(\n",
    "            f\"books/pausanias_book_{current_book}.xml\",\n",
    "            xml_declaration=True,\n",
    "            encoding=\"utf-8\",\n",
    "            pretty_print=True\n",
    "        )\n",
    "\n",
    "        tei = etree.Element(\"TEI\", nsmap=NSMAP, version=\"3.3.0\")\n",
    "        standoff = etree.SubElement(tei, \"standOff\", type=\"recogito_studio_annotations\")\n",
    "        listannotation = etree.SubElement(standoff, \"listAnnotation\")\n",
    "        text = etree.SubElement(tei, \"text\")\n",
    "        body = etree.SubElement(text, \"body\")\n",
    "\n",
    "        counter = 1\n",
    "        chapter = 0 \n",
    "\n",
    "        head = etree.SubElement(body, \"head\")\n",
    "        head.text = f\"Book {i.get(\"book\")}\"\n",
    "\n",
    "    current_book = i.get(\"book\")\n",
    "\n",
    "    if i.get(\"chapter\") != chapter:\n",
    "        head = etree.SubElement(body, \"head\")\n",
    "        head.text = f\"Chapter {i.get(\"chapter\")}\"\n",
    "        chapter = i.get(\"chapter\")\n",
    "\n",
    "    p = etree.SubElement(body, \"p\")\n",
    "    p.text = i.get(\"text\")\n",
    "    for mention in i.get(\"mentions_tagged\"):\n",
    "        annotation = etree.SubElement(listannotation, \"annotation\", target=f\"/TEI[1]/text[1]/body[1]/p[{str(counter)}]::{str(mention.get(\"start\"))} /TEI[1]/text[1]/body[1]/p[{str(counter)}]::{str(mention.get(\"end\"))}\")\n",
    "        annotation.set(f\"{{{NS_XML}}}id\", f\"UID-FAKE-{uid_counter}\")\n",
    "\n",
    "        # Comment this if you didn't run the optional step\n",
    "        topos_id = list(mention.get(\"reranker\")[0].keys())[0]\n",
    "        \n",
    "        # Uncomment this if you didn't run the optional step\n",
    "        #topos_id = mention.get(\"vector_db\")[0][0]\n",
    "\n",
    "        rs = etree.SubElement(annotation, \"rs\", ana=f\"https://topostext.org/place/{topos_id}\")\n",
    "        uid_counter += 1\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "tree = etree.ElementTree(tei)\n",
    "tree.write(\n",
    "    f\"books/pausanias_book_{current_book}.xml\",\n",
    "    xml_declaration=True,\n",
    "    encoding=\"utf-8\",\n",
    "    pretty_print=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
